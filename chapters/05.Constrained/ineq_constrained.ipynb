{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Inequality constrained convex optimization\n\n## Inequality constrained minimization problems\n\n$$\n\\min_\\mathbf{x}  f_0(\\mathbf{x})\n$$\n$$\n\\text{subject to } f_i(\\mathbf{x}) \\leq 0, \\quad i=1,\\ldots,m\n$$\n$$\nA\\mathbf{x}=\\mathbf{b}\n$$\nwhere $f_0,\\ldots,f_m\\ :\\ \\mathbb{R}^n \\rightarrow \\mathbb{R}$ are convex and twice continuously differentiable, and $A\\in \\mathbb{R}^{p\\times n}$ with **rank** $A=p<n$.\n\n**Question 1**\n\nWrite the inequality constraints for the case when:\n\n1. $x_k\\geq 0$\n2. $\\mathbf{x}$ lies in the unit sphere.\n\nUsing the theory of Lagrange multipliers, a point $\\mathbf{x}^\\star \\in\\mathbb{R}^n$ is optimal if and only if there exist a $\\boldsymbol{\\lambda}^\\star\\in \\mathbb{R}^m$ and $\\boldsymbol{\\nu}^\\star\\in \\mathbb{R}^p$ such that\n$$\nA\\mathbf{x}^\\star=\\mathbf{b}\n$$\n$$\nf_i(\\mathbf{x}^\\star) \\leq 0, \\quad i=1,\\ldots,m\n$$\nand\n$$\n\\lambda_i^\\star \\geq 0, \\quad i=1,\\ldots,m\n$$\n$$\n\\nabla f_0(\\mathbf{x}^\\star)+\\sum_{i=1}^m\\lambda^\\star_i\\nabla f_i(\\mathbf{x}^\\star) +A^\\top \\boldsymbol{\\nu}^\\star=0\n$$\n$$\n\\lambda_if_i(\\mathbf{x}^\\star)=0, \\quad i=1,\\ldots,m\\,.\n$$\n\n**Example**\n\nThe non-quadratic function with inequality constraints:\n\n$$\n\\min_\\mathbf{x} f_0(x_1, x_2)   = \\log(e^{x_1 +3x_2-0.1}+e^{x_1 -3x_2-0.1}+e^{-x_1 -0.1})\n$$\n$$\n \\text{subject to }  (x_1 - 1)^2 + (x_2 - 0.25)^2 \\leq 1\n$$\n\n![Convex function with an equality constraint. Note that the feasible region is a convex set.](Figures/ineq_const_example.png)\n\n## Implicit constraints\n\nRather than solving a minimization problems with inequality constraints, we can reformulate the objective function to include only the feasible regions:\n\n$$\n\\min_{\\mathbf{x}} f_0(\\mathbf{x})+\\sum_{i=1}^m I_{-}(f_i(\\mathbf{x}))\n$$\n$$\nA\\mathbf{x}=\\mathbf{b}\n$$\nwhere $I_{-}:\\mathbb{R}\\rightarrow \\mathbb{R}$ is the *indicator function* for the nonpositive reals:\n$$\nI_-(u) = 0 \\text{ if } u\\leq 0\n$$\nand\n$$\nI_-(u) = \\infty \\text{ if } u> 0\\,.\n$$\nSadly, we cannot directly optimize such a function using gradient-based optimization as $I_-$ does not provide gradients to guide us.\n\n## Logarithmic barrier\n\nMain idea: approximate $I_-$ by the function:\n\n$$\n\\hat{I}_-(u) = - (1/t)\\log(-u) \\text{ if } u< 0\n$$\nand\n$$\n\\hat{I}_-(u)=\\infty  \\text{ if } u\\geq 0\n$$\nwhere $t>0$ is a parameter that sets the accuracy of the approximation.\n\nThus the problem can be approximated by:\n\n$$\n\\min_\\mathbf{x} f_0(\\mathbf{x}) +\\sum_{i=1}^m\\hat{I}_-(f_i(\\mathbf{x}))\n$$\n$$\n\\text{subject to } A\\mathbf{x}=\\mathbf{b}\\,.\n$$\nNote that:\n\n- since $\\hat{I}_-(u)$ is convex and  increasing in $u$, the objective is also convex;\n- unlike the function $I$, the function $\\hat{I}_-(u)$ is differentiable;\n- as $t$ increases, the approximation becomes more accurate, as shown below.\n\n![Larger values of $t$ result in a better approximation of](Figures/log_bar.png)\n\n## The barrier method\n\nThe function\n\n$$\n\\phi (\\mathbf{x}) =\\sum_{i=1}^m-\\log(-f_i(\\mathbf{x}))\\,\n$$\n\nis called the *logarithmic barrier* for the constrained optimization problem.\n\nThe new optimization problem becomes:\n$$\n\\min_\\mathbf{x} tf_0(\\mathbf{x}) +\\phi (\\mathbf{x})\n$$\n$$\n\\text{subject to } A\\mathbf{x}=\\mathbf{b}\\,.\n$$\n\n- The parameter $t$ determines the quality of the approximation, the higher the value the closer the approximation matches the original problem.\n- The drawback of higher values of $t$ is that the problem becomes harder to optimize using Newton's method, as its Hessian will vary rapidly near the boundary of the feasible set.\n- This can be circumvented by solving a sequence of problems with increasing $t$ at each step, starting each Newton minimization at the solution of the previous value of $t$.\n\nComputed for you:\n\n- gradient of $\\phi$:\n$$\n\\nabla\\phi(\\mathbf{x}) = \\sum_{i=1}^m\\frac{1}{-f_i(\\mathbf{x})} \\nabla f_i(\\mathbf{x})\n$$\n- Hessian of $\\phi$:\n$$\n\\nabla^2\\phi(\\mathbf{x}) = \\sum_{i=1}^m \\frac{1}{f_i(\\mathbf{x})^2} \\nabla f_i(\\mathbf{x}) \\nabla f_i(\\mathbf{x})^\\top+\\sum_{i=1}^m\\frac{1}{-f_i(\\mathbf{x})^2} \\nabla^2 f_i(\\mathbf{x})\n$$\n\nThe pseudocode of the **barrier method** is given below. We start with a low value of $t$ and increase every step with a factor $\\mu$ until $m/t$ is smaller than some $\\epsilon>0$.\n\n\n>**input** strictly feasible $\\mathbf{x}$, $t:=t^{(0)}>0, \\mu>1$, tolerance $\\epsilon>0$.\n>\n>**repeat**\n>\n>>    1. *Centering step*.<br>\n>>   Compute $\\mathbf{x}^\\star(t)$ by minimizing $tf_0+\\phi$, subject to $A\\mathbf{x}=\\mathbf{b}$, starting at $\\mathbf{x}$.\n>>    2. *Update*. $\\mathbf{x}:=\\mathbf{x}^\\star(t)$\n>>    3. *Stopping criterion*. **quit** if $m/t<\\epsilon$.\n>>    4. *Increase $t$.*  $t:=\\mu t$.\n>\n>**until** $m/t < \\epsilon$\n>\n>**output** $\\mathbf{x}$\n\n**Choice of $\\mu$**\n\nThe choice has a trade-off in the number of inner and outer iterations required:\n- If $\\mu$ is small (close to 1) then $t$ increases slowly. A large number of Newton iterations will be required, but each will go fast.\n- If $\\mu$ is large then $t$ increases very fast. Each Newton step will take a long time to converge, but few iterations will be needed.\n\nThe exact value of $\\mu$ is not particularly critical, values between 10 and 20 work well.\n\n**Choice of $t^{(0)}$**\n\n- If $t^{(0)}$ is chosen too large: the first outer iteration will require many iterations.\n- If $t^{(0)}$ is chosen too small: the algorithm will require extra outer iterations.\n\n\n## Central path\n\nThe *central path* is the set of points satisfying:\n\n-  $\\mathbf{x}^\\star(t)$ is strictly feasible: $A\\mathbf{x}^\\star(t)=\\mathbf{b}$ and $f_i(\\mathbf{x}^\\star(t))<0$ for $i=1,\\ldots,m$\n- there exist $\\hat{\\boldsymbol{\\nu}}\\in\\mathbb{R}^p$ such that\n$$\nt\\nabla f_0(\\mathbf{x}^\\star(t)) + \\nabla \\phi(\\mathbf{x}^\\star(t)) +A^\\top \\hat{\\boldsymbol{\\nu}}=0\n$$\n- one can show that $f_0(\\mathbf{x}^\\star(t))-p^\\star\\leq m / t$: $f_0(\\mathbf{x}^\\star(t))$ converges to an optimal point as $t\\rightarrow \\infty$.\n\n# References\n\n- Boyd, S. and Vandenberghe, L., '*[Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)*'. Cambridge University Press (2004)\n- Bishop, C., *Pattern Recognition and Machine Learning*. Springer (2006)"
      ],
      "metadata": {}
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.6.1"
    },
    "kernelspec": {
      "name": "julia-1.6",
      "display_name": "Julia 1.6.1",
      "language": "julia"
    }
  },
  "nbformat": 4
}
