{
  "cells": [
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Plots\nusing LinearAlgebra\nusing STMO"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Motivation\n\nMany more realistic optimization problems are characterized by constraints. For example, real-word systems often satisfy conservation laws, such as conservation of mass, of atoms or of charge. When designing objects, there are practical constraints of feasible dimensions, range of operations and limitations in materials. Another example is in probability, where a solution should satisfy the axioms of probability theory (probabilities are real values between 0 and 1 and the probabilities of all events should sum to 1).\n\nOther cases, we include constraints in our problem because they encode prior knowledge about the problem or to obtain solutions with certain desirable properties.\n\nIn this chapter we discuss convex optimization problems with linear equality constraints (constraining the solution to a linear subspace) and convex inequality constrains (constraining the solution to convex subspace). Both types of constraints result again in a convex optimization problem.\n\n# Lagrange multipliers\n\nLagrange multipliers are elegant ways of finding stationary points of a function of several variables given one or more constraints. We give a short introduction based on a geometric perspective.\n\n> IMPORTANT: most textbooks treat Lagrange multipliers from as maximization problems. Here they are treated as minimization problems to be consistent with other chapters.\n\n## Equality constraints\n\nConsider the following optimization problem:\n\n$$\n\\min_{\\mathbf{x}} f(\\mathbf{x})\n$$\n$$\n\\text{subject to } g(\\mathbf{x})=0\\,.\n$$\n\n![Convex optimization problem with an equality constraint. Here, the constraint is nonlinear.](Figures/Lagr1.png)\n\nFor every point $\\mathbf{x}$ on the surface $g(\\mathbf{x})=0$, the gradient $\\nabla g(\\mathbf{x})$ is normal to this surface. This can be shown by considering a point $\\mathbf{x}+\\boldsymbol{\\epsilon}$, also on the surface. If we make a Taylor expansion around $\\mathbf{x}$, we have\n\n$$\ng(\\mathbf{x}+\\boldsymbol{\\epsilon})\\approx g(\\mathbf{x}) + \\boldsymbol{\\epsilon}^\\top\\nabla g(\\mathbf{x})\\,.\n$$\n\nGiven that both $\\mathbf{x}$ and $\\mathbf{x}+\\boldsymbol{\\epsilon}$ lie on the surface it follows that $g(\\mathbf{x}+\\boldsymbol{\\epsilon})= g(\\mathbf{x})$. In the limit that $||\\boldsymbol{\\epsilon}||\\rightarrow 0$ we have that $\\boldsymbol{\\epsilon}^\\top\\nabla g(\\mathbf{x})=0$. Because $\\boldsymbol{\\epsilon}$ is parallel to the surface $g(\\mathbf{x})$, it follows that $\\nabla g(\\mathbf{x})$ is normal to the surface.\n\n![The same optimization problem, with some gradients of $f(\\mathbf{x})$ and $g(\\mathbf{x})$ shown.](Figures/Lagr2.png)\n\nWe seek a point $\\mathbf{x}^\\star$ on the surface such that $f(\\mathbf{x})$ is minimized. For such a point, it should hold that the gradient w.r.t. $f$ should be parallel to $\\nabla g$. Otherwise, it would be possible to give a small 'nudge' to $\\mathbf{x}^\\star$ in the direction of $\\nabla f$ to decrease the function value, which would indicate that $\\mathbf{x}^\\star$ is not a minimizer. This figures below illustrate this point.\n\n![Point on the surface that is *not* a minimizer.](Figures/Lagr3.png)\n\n![Point on the surface that is a minimizer of $f$.](Figures/Lagr4.png)\n\n$$\n\\nabla f(\\mathbf{x}^\\star) + \\nu \\nabla g (\\mathbf{x}^\\star)=0\\,,\n$$\nwith $\\nu\\neq 0$ called the *Lagrange multiplier*. The constrained minimization problem can also be represented by a *Lagrangian*:\n$$\nL(\\mathbf{x}, \\nu) \t\\equiv f(\\mathbf{x}) + \\nu g(\\mathbf{x})\\,.\n$$\nThe constrained stationary condition is obtained by setting $\\nabla_\\mathbf{x} L(\\mathbf{x}, \\nu) =0$, the condition $\\partial  L(\\mathbf{x}, \\nu)/\\partial \\nu=0$ leads to the constraint equation $g(\\mathbf{x})=0$.\n\n## Inequality constraints\n\nThe same argument can be made for inequality constraints, i.e. solving\n\n$$\n\\min_{\\mathbf{x}} f(\\mathbf{x})\n$$\n$$\n\\text{subject to } g(\\mathbf{x})\\leq0\\,.\n$$\n\nHere, two situations can arise:\n\n- **Inactive constraint**: the minimizer of $f$ lies in the region where $g(\\mathbf{x}) < 0$. This corresponds to a Lagrange multiplier $\\nu=0$. Note that the solution would be the same if the constraint was not present.\n- **Active constraint**: the minimizer of $f$ lies in the region where $g(\\mathbf{x}) > 0$. The solution of the constrained problem will lie on the bound where $g(\\mathbf{x})=0$, similar to the equality-constrained problem and corresponds to a Lagrange multiplier $\\nu>0$.\n\nBoth scenarios are shown below:\n\n![Constrained minimization problem with an active inequality constraint. Optimum lies within the region where $g(\\mathbf{x})\\leq 0$. ](Figures/Lagr6.png)\n\n![Constrained minimization problem with an active inequality constraint. Optimum lies on the boundary of the region where $g(\\mathbf{x})\\leq 0$.](Figures/Lagr5.png)\n\n\nFor both cases, the product $\\nu g(\\mathbf{x})=0$, the solution should thus satisfy the following conditions:\n$$\ng(\\mathbf{x}) \\leq 0\n$$\n$$\n\\nu \\geq 0\n$$\n$$\n\\nu g(\\mathbf{x})=0\\,.\n$$\nThese are called the *Karush-Kuhn-Tucker* conditions.\n\nIt is relatively straightforward to extend this framework towards multiple constraints (equality and inequality) by using several Lagrange multipliers.\n\n# Equality constrained convex optimization\n\n## Problem outline\n\nWe will start with convex optimization problems with linear equality constraints:\n\n$$\n\\min_\\mathbf{x} f(\\mathbf{x})\n$$\n$$\n\\text{subject to } A\\mathbf{x}=\\mathbf{b}\n$$\n\nwhere $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is convex and twice continuously differentiable and $A\\in \\mathbb{R}^{p\\times n}$ with a rank $p < n$.\n\nThe Lagrangian of this problem is\n\n$$\nL(\\mathbf{x}, \\boldsymbol{\\nu}) = f(\\mathbf{x}) + \\boldsymbol{\\nu}^\\top(A\\mathbf{x}-\\mathbf{b})\\,,\n$$\nwith $\\boldsymbol{\\nu}\\in\\mathbb{R}^p$ the vector of Lagrange multipliers.\n\nA point $\\mathbf{x}^\\star\\in$ **dom** $f$ is optimal for the above optimization problem only if there is a $\\boldsymbol{\\nu}^\\star\\in\\mathbb{R}^p$ such that:\n\n$$\nA\\mathbf{x}^\\star = \\mathbf{b}, \\qquad \\nabla f(\\mathbf{x}^\\star) + A^\\top\\boldsymbol{\\nu}^\\star = 0\\,.\n$$\n\nWe will reuse the same toy examples from the previous chapter, but add an equality constraint to both.\n\n- Simple quadratic problem:\n\n$$\n \\min_{\\mathbf{x}} \\frac{1}{2} (x_1^2 + 4 x_2^2)\n$$\n$$\n \\text{subject to }  x_1 - 2x_2 = 3\n$$\n\n- A non-quadratic function:\n\n$$\n \\min_{\\mathbf{x}}\\log(e^{x_1 +3x_2-0.1}+e^{x_1 -3x_2-0.1}+e^{-x_1 -0.1})\n$$\n$$\n \\text{subject to }  x_1 + 3x_2 = 0\n$$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "import STMO.TestFuns: fquadr, grad_fquadr, hess_fquadr\nimport STMO.TestFuns: fnonquadr, grad_fnonquadr, hess_fnonquadr"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "![The two toy functions each with a linear constraint.](Figures/example_functions.png)\n\n## Equality constrained convex quadratic optimization\n\nConsider the following equality constrained convex optimization problem:\n\n$$\n\\min_\\mathbf{x}\\frac{1}{2}\\mathbf{x}^\\top P \\mathbf{x} + \\mathbf{q}^\\top \\mathbf{x} + r\n$$\n$$\n\\text{subject to }  A\\mathbf{x}=\\mathbf{b}\n$$\n\nwhere $P$ is symmetric.\n\nThe optimality conditions are\n$$\nA\\mathbf{x}^\\star = \\mathbf{b}, \\quad P\\mathbf{x}^\\star+\\mathbf{q} +A^\\top\\boldsymbol{\\nu}^\\star=\\mathbf{0}\\,,\n$$\nwhich we can write as\n\n$$\n\\begin{bmatrix}\nP & A^\\top \\\\\nA & 0 \\\\\n     \\end{bmatrix}\n     \\begin{bmatrix}\n\\mathbf{x}^\\star\\\\\n\\boldsymbol{\\nu}^\\star\n     \\end{bmatrix}\n     =\n     \\begin{bmatrix}\n-\\mathbf{q} \\\\\n\\mathbf{b}\n     \\end{bmatrix}\\,.\n$$\nNote that this is a block matrix.\n\n> If $P$ is positive-definite, the linearly constrained quadratic minimization problem has an unique solution.\n\nSolving this linear system gives both the constrained minimizer $\\mathbf{x}^\\star$ as well as the Lagrange multipliers.\n\n**Assignment 1**\n\n1. Complete the code to solve linearly constrained quadratic systems.\n2. Use this code to solve the quadratic toy problem defined above."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "\"\"\"\n    solve_constrained_quadratic_problem(P, q, A, b)\n\nSolve a linear constrained quadratic convex problem.\n\nInputs:\n    - P, q: quadratic and linear parameters of\n            the linear function to be minimized\n    - A, b: system of the linear constraints\n\nOutputs:\n    - xstar: the exact minimizer\n    - nustar: the optimal Lagrange multipliers\n\"\"\"\nfunction solve_constrained_quadratic_problem(P, q, A, b)\n    p, n = ...  # size of the problem\n    # complete this code\n    solution = ...\n    xstar = ...\n    nustar = ...\n    return xstar, nustar\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# solve the quadratic system with the linear constraint"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Newton's method with equality constraints\n\nTo derive $\\Delta \\mathbf{x}_{nt}$ for the following equality constrained problem\n\n$$\n\\min_\\mathbf{x}  f(\\mathbf{x})\n$$\n$$\n\\text{subject to }  A\\mathbf{x}=\\mathbf{b}\n$$\n\nwe apply a second-order Taylor approximation at the point $\\mathbf{x}$, to obtain\n\n$$\n\\min_\\mathbf{v} \\hat{f}(\\mathbf{x} +\\mathbf{v}) = f(\\mathbf{x}) +\\nabla f(\\mathbf{x})^\\top \\mathbf{v}+ \\frac{1}{2}\\mathbf{v}^\\top \\nabla^2 f(\\mathbf{x}) \\mathbf{v}\n$$\n$$\n\\text{subject to } A(\\mathbf{x}+\\mathbf{v})=\\mathbf{b}\\,.\n$$\n\nBased on the solution of quadratic convex problems with linear constraints, the Newton $\\Delta \\mathbf{x}_{nt}$ step is characterized by\n\n$$\n\\begin{bmatrix}\n \\nabla^2 f(\\mathbf{x})&  A^\\top \\\\\nA & 0 \\\\\n     \\end{bmatrix}\n     \\begin{bmatrix}\n\\Delta \\mathbf{x}_{nt}\\\\\n\\mathbf{w}\n     \\end{bmatrix}\n     =\n     -\\begin{bmatrix}\n\\nabla f(\\mathbf{x}) \\\\\nA\\mathbf{x}-\\mathbf{b}\n     \\end{bmatrix}\n$$\n\n- If the starting point $\\mathbf{x}^{(0)}$ is chosen such that $A\\mathbf{x}^{(0)}=\\mathbf{b}$, the residual term vanishes and steps will remain in the feasible region. This is the **feasible start Newton method**.\n- If we choose an arbitrary $\\mathbf{x}^{(0)}\\in$ **dom** $f$, not satisfying the constraints, this is the **infeasible start Newton method**. It will usually converge rapidly to the feasible region (check the final solution!).\n\nNote that when we start at a feasible point, the residual vector $-(A\\mathbf{x}-\\mathbf{b})$ vanishes and the path will always remain in a feasible region. Otherwise we will converge to it.\n\nIn this chapter, we will use a fixed step size. For Newton's method this usually leads to only a few extra iterations compared to an adaptive step size.\n\n>**input** starting point $\\mathbf{x}\\in$ **dom** $f$ (with $A\\mathbf{x}=\\mathbf{b}$ if using the feasible method), tolerance $\\epsilon>0$.\n>\n>**repeat**\n>\n>>    1. Compute the Newton step $\\Delta \\mathbf{x}_{nt}$ and decrement $\\lambda(\\mathbf{x})$.\n>>    2. *Stopping criterion*. **break** if $\\lambda^2/2\\leq \\epsilon$.\n>>    3. *Choose step size $t$*: either by line search or fixed $t$.\n>>    4. *Update*. $\\mathbf{x}:=\\mathbf{x}+t \\Delta \\mathbf{x}_{nt}$.\n>\n>**output** $\\mathbf{x}$\n\nAgain, the convergence can be monitored using the Newton decrement:\n\n$$\n\\lambda^2(\\mathbf{x}) = \\Delta \\mathbf{x}_{nt}^\\top \\nabla^2 f(\\mathbf{x})\\Delta \\mathbf{x}_{nt}\\,.\n$$\n\nThe algorithm terminates when\n\n$$\n\\frac{\\lambda(\\mathbf{x})^2}{2} < \\epsilon\\,.\n$$\n\nThe Newton decrement also indicates whether we are in or close to the feasible region.\n\n**Assignment 2**\n\n1. Complete the code for the linearly constrained Newton method.\n2. Use this code to find the minimum of the non-quadratic toy problem, defined above (compare a feasible and infeasible start)."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "\"\"\"\nlinear_constrained_newton(f, x₀, Df,\n              DDf, A, b; t::Real=0.25, ϵ::Real=1e-3,\n              tracker::Tracker=notrack)\n\nNewton's method for minimizing functions with linear constraints.\nExpects a feasible x₀!\n\nInputs:\n    - f: function to be minimized\n    - x₀: starting point (does not have to be feasible)\n    - Df: gradient of the function to be minimized\n    - DDf: hessian matrix of the function to be minimized\n    - A, b: linear constraints\n    - t: step size for each Newton step (fixed)\n    - ϵ: parameter to determine if the algorithm is converged\n    - verbose: print the number of steps?\n\nOutputs:\n    - xstar: the found minimum\n\"\"\"\nfunction linear_constrained_newton(f, x₀, Df,\n              DDf, A, b; t::Real=0.25, ϵ::Real=1e-4, verbose=false)\n    @assert 0.0 < t <= 1.0 \"stepsize t should be in (0, 1]\"\n    @assert ... # test if x₀ is feasible\n    x = x₀  # initial value\n    p, n = size(A)\n    nsteps = 0\n    while true\n        dfx = Df(x)\n        ddfx = DDf(x)\n        # calculate residual\n        Dx, _ = solve_constrained_quadratic_problem(...) # complete!\n        λ² = ...\n        if ...  # stopping criterion\n            break  # converged\n        end\n        # perform step\n        ...\n        nsteps += 1\n    end\n    verbose && println(\"Converged in $nsteps steps\")\n    return x\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# solve the nonlinear system"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n\n- Boyd, S. and Vandenberghe, L., '*[Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)*'. Cambridge University Press (2004)\n- Bishop, C., *Pattern Recognition and Machine Learning*. Springer (2006)"
      ],
      "metadata": {}
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.6.1"
    },
    "kernelspec": {
      "name": "julia-1.6",
      "display_name": "Julia 1.6.1",
      "language": "julia"
    }
  },
  "nbformat": 4
}
