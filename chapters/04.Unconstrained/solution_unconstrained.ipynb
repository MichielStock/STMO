{
  "cells": [
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Plots, LaTeXStrings\nusing LinearAlgebra\nusing STMO"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Motivation\n\nIn this chapter we will study unconstrained convex problems, i.e., problems of the form\n\n$$\n\\min_\\mathbf{x}\\, f(\\mathbf{x})\\,,\n$$\n\nin which $f$ is *convex*. Convex optimization problems are well understood. Their most attractive property is that when a minimizer exists, the minimizer is the unique global minimizer.\n\nMost convex optimization problems do not have a closed-form solution, with the quadratic problems of the previous chapters as a notable exception. We will hence again have to resort to descent methods to find an (arbitrary accurate) approximate solution.\n\n## Convex sets and functions\n\n### Convex set\n\n> **In words**: a set $\\mathcal{C}$ is called *convex* if the line segment between any two points in $\\mathcal{C}$ also lies in $\\mathcal{C}$.\n\n> **In symbols**:  a set $\\mathcal{C}$ is called *convex* if, for any $\\mathbf{x}, \\mathbf{x}' \\in \\mathcal{C}$ and any $\\theta \\in [0, 1]$, it holds that $\\theta \\mathbf{x} + (1 - \\theta) \\mathbf{x}' \\in \\mathcal{C}$.\n\n![Some convex (A & B) and non-convex sets (B & D).](Figures/convex_sets.png)\n\n### Convex functions\n\n> **In words**:  a function $f$ is *convex* if the line segment between $(\\mathbf{x}, f(\\mathbf{x}))$ and $(\\mathbf{x}', f (\\mathbf{x}'))$ lies above the graph of $f$.\n\n> **In symbols**: a function $f : \\mathbb{R}^n\\rightarrow \\mathbb{R}$ is *convex* if\n> - dom($f$) is convex\n> - for any $\\mathbf{x}, \\mathbf{x}' \\in \\text{dom}(f)$ and any $\\theta \\in [0, 1]$, it holds that $f(\\theta \\mathbf{x} + (1-\\theta)\\mathbf{x}') \\leq\\theta f(\\mathbf{x}) +(1-\\theta)f(\\mathbf{x}')$.\n\nBelow is an example of a convex function."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x) = 0.1x^4 - 2x + x^2\n\nplot(f, -4:0.1:4, xlabel=\"\\$f(x)\\$\", color=myblue, lw=2)\n\nx, x′ = -3.5, 2.75\nscatter!([x, x′], f.([x, x′]), label=\"\\$x, x'\\$\", color=mygreen)\nplot!([x, x′], f.([x, x′]), label=\"\\$(1-\\\\theta)f(x)+\\\\theta f(x')\\$\", color=myred, lw=2)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Some convex (A & C) and non-convex functions (B).](Figures/convex_functions.png)\n\nFrom the definition, it follows that:\n\n- If the function is differentiable, then $f(\\mathbf{x})\\geq f(\\mathbf{x}')+\\nabla f(\\mathbf{x}')^\\top(\\mathbf{x}-\\mathbf{x}')$ for all $\\mathbf{x}$ and $\\mathbf{x}' \\in \\text{dom}(f)$. **The first-order Taylor approximation is a global underestimator of $f$.**\n- If the function is twice differentiable, then $\\nabla^2 f(\\mathbf{x})\\succeq 0$ for any $\\mathbf{x}\\in\\text{dom}(f)$.\n\nConvex functions frequently arise:\n\n- If $f$ and $g$ are both convex, then $m(x)=\\max(f(x), g(x))$ and $h(x)=f(x)+g(x)$ are also convex.\n- If $f$ and $g$ are convex functions and $g$ is non-decreasing over a univariate domain, then $h(x)=g(f(x))$ is convex. Example: $e^{f(x)}$ is convex if $f({x})$ is convex.\n- If $f$ is concave and g is convex and non-increasing over a univariate domain, then ${\\displaystyle h(x)=g(f(x))}$ is convex.\n\nNote, the convexity of expected value in probability theory gives rise to *Jensen's inequality*. For any convex function $\\varphi$, if holds that\n$$\n\\varphi(\\mathbb{E}[X]) \\leq\\mathbb{E}[\\varphi(X)]\\,.\n$$\n\nThis implies for example that the square of an expected value of quantity is never greater than the expected square of that quantity.\n\n### Strongly convex functions\n\n> **In words**: a function $f$ is called *strongly convex* if it is at least as convex as a quadratic function.\n\n> **In symbols**: a $f$ is called *strongly $m$-convex* (with $m>0$) if the function $f_m(\\mathbf{x}) = f(\\mathbf{x}) - \\frac{m}{2}||\\mathbf{x}||_2$ is convex.\n\nIf the first- and second order derivatives exists, a strongly $m$-convex function satisfies:\n\n-  $f(\\mathbf{x}') \\geq f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^\\top (\\mathbf{x}'-\\mathbf{x}) + \\frac{m}{2}||\\mathbf{x}'-\\mathbf{x}||_2$\n-  $\\nabla^2 f(\\mathbf{x})-mI\\succeq 0$ (all eigenvalues of the Hessian are greater than $m$)\n\nIf a function is $m$-strongly convex, this also implies that there exists an $M>m$ such that\n\n$$\n\\nabla^2 f(\\mathbf{x}) \\preceq MI\\,.\n$$\n\nStated differently, for strongly convex functions the exist both a quadratic function with a smaller as well as a lower local curvature.\n\n![For strongly convex functions, it holds that there are two constants $m$ and $M$ such that $mI\\preceq\\nabla^2 f(\\mathbf{x}) \\preceq MI$. ](Figures/strong_convexity.png)\n\n\n## Toy examples\n\nTo illustrate the algorithms, we introduce two toy functions to minimize:\n\n- simple quadratic problem:\n$$\nf(x_1, x_2) = \\frac{1}{2} (x_1^2 +\\gamma x_2^2)\\,,\n$$\nwhere $\\gamma$ determines the condition number.\n- a non-quadratic function:\n$$\nf(x_1, x_2) = \\log(e^{x_1 +3x_2-0.1}+e^{x_1 -3x_2-0.1}+e^{-x_1 -0.1})\\,.\n$$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "import STMO.TestFuns: fquadr, grad_fquadr, hess_fquadr\nimport STMO.TestFuns: fnonquadr, grad_fnonquadr, hess_fnonquadr"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "pq = contour(-10:0.1:10, -5:0.1:5, (x1, x2) -> fquadr((x1, x2)), xlabel=\"\\$ x_1 \\$\",\n                ylabel=\"\\$ x_2 \\$\", title=\"quadratic\", fill=false)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "pnq = contour(-2:0.1:2, -1:0.1:1, (x1, x2) -> fnonquadr((x1, x2)), xlabel=\"\\$ x_1 \\$\",\n                ylabel=\"\\$ x_2 \\$\", title=\"non-quadratic\", fill=false)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General descent methods (recap)\n\nConvex functions are usually minimized using descent methods. Again, line search is often used as a subroutine.\n\nThe outline of a general descent algorithm is given in the following pseudocode.\n\n> **input** starting point $\\mathbf{x}\\in$ **dom** $f$.\n>\n> **repeat**\n>\n>>    1. Determine a descent direction $\\Delta \\mathbf{x}$.\n>>    2. *Line seach*. Choose a step size $t>0$.\n>>    3. *Update*. $\\mathbf{x}:=\\mathbf{x}+t\\Delta \\mathbf{x}$.\n>\n> **until** stopping criterion is satisfied.\n>\n> **output** $\\mathbf{x}$\n\n\nThe specific optimization algorithms are hence determined by:\n\n- method for determining the search direction $\\Delta \\mathbf{x}$, this is almost always based on the gradient of $f$\n- method for choosing the step size $t$, may be fixed or adaptive\n- the criterion used for terminating the descent, usually the algorithm stops when the improvement is smaller than a predefined value\n\n## Backtracking line search\n\nFor quadratic optimization, as covered in Chapter 1, the optimal step size could be computed in closed form. In the general case, only an approximately optimal step size is used.\n\n### Exact line search\n\nAs a subroutine of the general descent algorithm a line search has to be performed. A value for $t$ is chosen to minimize $f$ along the ray $\\{\\mathbf{x}+t\\Delta \\mathbf{x} \\mid t\\geq0\\}$:\n\n$$\nt = \\text{arg min}_{s\\geq0}\\ f(\\mathbf{x}+s\\Delta \\mathbf{x})\\,.\n$$\n\nExact line search is used when the cost of solving the above minimization problem is small compared to the cost of calculating the search direction itself. This is sometimes the case when an analytical solution is available.\n\n### Inexact line search\n\nOften, the descent methods work well when the line search is done only approximately. This is because the computational resources are better spent to performing more *approximate* steps in the different directions because the direction of descent will change anyway.\n\nMany methods exist for this, we will consider the *backtracking line search* (BTLS), described by the following pseudocode.\n\n> **input** starting point $\\mathbf{x}\\in$ **dom** $f$, descent direction $\\Delta \\mathbf{x}$, gradient $\\nabla f(\\mathbf{x})$,  $\\alpha\\in(0,0.5)$ and $\\beta\\in(0,1)$.\n>\n>  $t:=1$\n>\n>**while** $f(\\mathbf{x}+t\\Delta \\mathbf{x}) > f(x) +\\alpha t \\nabla f(\\mathbf{x})^\\top\\Delta \\mathbf{x}$\n>\n>>    $t:=\\beta t$\n>\n>\n>**output** $t$\n\nThe backtracking line search has two parameters:\n\n-  $\\alpha$: fraction of decrease in $f$ predicted by linear interpolation we accept\n-  $\\beta$: reduction of the step size in each iteration of the BTLS\n-  typically, $0.01 \\leq \\alpha \\leq 0.3$ and $0.1 \\leq \\beta < 1$\n\n![Illustration of the backtracking line search.](Figures/btls.png)\n\n**Assignment 1**\n\n1. Complete the code for the backtracking line search\n2. Use this function find the step size $t$ to (approximately) minimize $f(x) = x^2 - 2x - 5$ starting from the point $0$. Choose a $\\Delta x=10$."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "\"\"\"\n    backtracking_line_search(f, x, Δx, grad_f, α::Real=0.1,\n                        β::Real=0.7)\n\nUses backtracking for finding the minimum over a line.\n\nInputs:\n    - f: function to be searched over a line\n    - x: initial point\n    - Δx: direction to search\n    - grad_f: gradient of f\n    - α\n    - β\n\nOutput:\n    - t: suggested stepsize\n\"\"\"\nfunction backtracking_line_search(f, x, Δx, grad_f; α::Real=0.1,\n                        β::Real=0.7)\n    @assert 0 < α < 0.5 && 0 < β < 1 \"incorrect values for α and/or β\"\n    t = 1.0\n    θ = Δx' * grad_f(x)  # store these to avoid recomputing them\n    fx = f(x)\n    while f(x .+ t * Δx) > fx + α * t * θ # complete\n        t *= β  # complete\n    end\n    return t\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "fun(x) = x^2 - 2x - 5\ngrad_fun(x) = 2x - 2\nx = -0.0\nΔx = -grad_fun(x)\nt = backtracking_line_search(fun, x, Δx, grad_fun)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "plot(fun, -3, 3)\nvline!([x], label=\"x\")\nvline!([x + Δx * t], label=\"x + t * Dx\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1**\n\nDescribe the effect of $\\alpha$, $\\beta$ and $\\Delta \\mathbf{x}$. How can you perform a more precise search?\n\n## Gradient descent\n\nA natural choice for the search direction is the negative gradient: $\\Delta \\mathbf{x} = -\\nabla f(\\mathbf{x})$. This algorithm is called the *gradient descent algorithm*.\n\n### General gradient descent algorithm\n\n>**input** starting point $\\mathbf{x}\\in$ **dom** $f$.\n>\n>**repeat**\n>\n>>    1. *Choose direction*. $\\Delta \\mathbf{x} := -\\nabla f(\\mathbf{x})$.\n>>    2. *Line seach*. Choose a step size $t$ via exact or backtracking line search.\n>>    3. *Update*. $\\mathbf{x}:=\\mathbf{x}+t\\Delta \\mathbf{x}$.\n>\n>**until** stopping criterion is satisfied.\n>\n>**output** $\\mathbf{x}$\n\nThe stopping criterion is usually of the form $||\\nabla f(\\mathbf{x})||_2 \\leq \\nu$.\n\n### Convergence analysis\n\nThe notion of strongly convexity allows us to bound the function $f$ by two quadratic functions. As such we can reuse the convergence analysis of the previous chapter.\n\nIf $f$ is strongly convex (constants $m$ and $M$ exist such that $mI\\prec \\nabla^2 f(\\mathbf{x})\\prec MI$), it holds that $f(\\mathbf{x}^{(k)}) - p^*\\leq \\varepsilon$ after at most\n$$\n\\frac{\\log((f(\\mathbf{x}^{(0)}) - p^*)/\\varepsilon)}{\\log(1/c)}\n$$\niterations, where $c =1-\\frac{m}{M}<1$.\n\nWe conclude:\n\n- The number of steps needed for a given quality is proportional to the logarithm of the initial error.\n- To increase the accuracy with an order of magnitude, only a few more steps are needed.\n- Convergence is again determined by the *condition number* $M/m$. Note that for large condition numbers: $\\log(1/c)=-\\log(1-\\frac{m}{M})\\approx m/M$, so the number of required iterations increases linearly with increasing $M/m$.\n\n**Assignment 2**\n\n1. Complete the implementation of the gradient descent method.\n2. Plot the paths for the two toy problems. Use $\\mathbf{x}^{(0)}=[10,1]^\\top$ for the quadratic function and $\\mathbf{x}^{(0)}=[-0.5,0.9]^\\top$ for the non-quadratic function as starting points.\n3. Analyze the convergence."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "\"\"\"\n    gradient_descent(f, x₀, grad_f; α::Real=0.2, β::Real=0.7,\n        ν::Real=1e-3)\n\nGeneral gradient descent algorithm.\n\nInputs:\n    - f: function to be minimized\n    - x₀: starting point\n    - grad_f: gradient of the function to be minimized\n    - α: parameter for btls\n    - β: parameter for btls\n    - ν: parameter to determine if the algorithm is converged\n\nOutputs:\n    - xstar: the found minimum\n\"\"\"\nfunction gradient_descent(f, x₀, grad_f; α::Real=0.2, β::Real=0.7,\n      ν::Real=1e-7)\n    x = x₀  # initial value\n    Δx = similar(x)\n    nsteps = 0\n    while true\n        # note the in-place update of Δx!\n        Δx .= -grad_f(x) # choose direction\n        if norm(Δx) < ν\n            break  # converged\n        end\n        t = backtracking_line_search(f, x, Δx, grad_f, α=α, β=β)\n        x .+= t * Δx # do a step, INPLACE!\n        nsteps += 1\n    end\n    println(\"converged after $nsteps steps\")\n    return x\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x0q = [10.0, 1.0]\nxstarq = gradient_descent(fquadr, copy(x0q), grad_fquadr)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x0nq = [-0.5,0.9]\nxstarnq = gradient_descent(fnonquadr, copy(x0nq), grad_fnonquadr)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Steepest descent\n\nOptimize the first-order Taylor approximation of a function:\n\n$$\nf(\\mathbf{x}+\\mathbf{v}) \\approx \\hat{f}(\\mathbf{x}+\\mathbf{v}) =f(\\mathbf{x}) +\\nabla f(\\mathbf{x})^\\top \\mathbf{v}\\,.\n$$\n\nThe linear approximation $\\hat{f}$ can be made arbitrary negative if we can freely choose $\\mathbf{v}$! We have to constrain the *norm* of $\\mathbf{v}$.\n\n### Vector norms\n\nA norm on $\\mathbb{R}^n$ is a function $||\\cdot||:\\mathbb{R}^n\\rightarrow \\mathbb{R}$ with the following properties:\n\n-  $||\\mathbf{x}||>0$, for any $\\mathbf{x}\\in\\mathbb{R}^n$\n-  $||\\mathbf{x}+\\mathbf{y}|| \\leq ||\\mathbf{x}||+||\\mathbf{y}||$, for any $\\mathbf{x}, \\mathbf{y}\\in\\mathbb{R}^n$\n-  $||\\lambda \\mathbf{x}|| = |\\lambda|\\, ||\\mathbf{x}||$ for any $\\lambda \\in\\mathbb{R}$ and any $\\mathbf{x}\\in\\mathbb{R}^n$\n-  $||\\mathbf{x}||=0$ if and only if $\\mathbf{x}=0$\n\nFor example, for any $\\mathbf{x}\\in\\mathbb{R}^n$ and $p\\leq 1$:\n$$\n||\\mathbf{x}||_p = \\left(\\sum_{i=1}^n |x_i|^p\\right)^\\frac{1}{p}\\,.\n$$\n\n$||\\cdot||_1$ is often called the $L_1$ norm and $||\\cdot||_2$ the $L_2$ norm.\n\nConsider $P\\in \\mathbb{R}^{n\\times n}$ such that $P\\succ 0$. The  corresponding *quadratic norm*:\n$$\n||\\mathbf{z}||_P = (\\mathbf{z}^\\top P\\mathbf{z})^\\frac{1}{2}=||P^\\frac{1}{2}\\mathbf{z}||_2\\,.\n$$\nThe matrix $P$ can be used to encode prior knowledge about the scales and dependencies in the space that we want to search.\n\n### Dual norm\n\nLet $|| \\cdot ||$ be a norm on $\\mathbb{R}^n$. The associated dual norm:\n$$\n||\\mathbf{z}||_*=\\sup_{\\mathbf{x}} \\{\\mathbf{z}^\\top\\mathbf{x}\\mid ||\\mathbf{x}||\\leq 1\\}\\,.\n$$\n\nExamples:\n\n- the dual norm of $||\\cdot||_1$ is $||\\cdot||_\\infty$;\n- the dual norm of $||\\cdot||_2$ is $||\\cdot||_2$;\n- the dual norm of $||\\cdot||_P$ is defined by $||\\mathbf{z}||_*=||P^{-\\frac{1}{2}}\\mathbf{z}||$.\n\n### Steepest descent directions\n\n**Normalized steepest descent direction**:\n\n$$\n\\Delta x_\\text{nsd} = \\text{arg min}_\\mathbf{v}\\, \\{\\nabla f(\\mathbf{x})^T \\mathbf{v} \\mid ||\\mathbf{v}||\\leq 1 \\}\\,.\n$$\n\n**Unnormalized steepest descent direction**:\n\n$$\n\\Delta x_\\text{sd} = ||\\nabla f(\\mathbf{x})||_\\star \\Delta x_\\text{nsd} \\,.\n$$\n\nNote that we have\n$$\n\\nabla f(\\mathbf{x})^\\top \\Delta x_\\text{sd} = ||\\nabla f(\\mathbf{x})||_\\star \\nabla f(\\mathbf{x})^\\top\\Delta x_\\text{nsd} = -||\\nabla f(\\mathbf{x})||^2_\\star\\,,\n$$\nso this is a valid descent method.\n\n![Illustration of some descent directions based on different norms.](Figures/sd_gradients.png)\n\n### Coordinate descent algorithm\n\nUsing the $L_1$ norm results in coordinate descent. For every iteration in this algorithm, we descent in the direction of the dimension where the absolute value of the gradient is largest.\n\n>**input** starting point $\\mathbf{x}\\in$ **dom** $f$.\n>\n>**repeat**\n>\n>>    1. *Direction*. Choose $i$ such that $|\\nabla f(\\mathbf{x})_i|$ is maximal.\n>>    2. *Choose direction*. $\\Delta \\mathbf{x} := -\\nabla f(\\mathbf{x})_i \\mathbf{e}_i$\n>>    3. *Line seach*. Choose a step size $t$ via exact or backtracking line search.\n>>    4. *Update*. $\\mathbf{x}:=\\mathbf{x}+t\\Delta \\mathbf{x}$.\n>\n>**until** stopping criterion is satisfied.\n>\n>**output** $\\mathbf{x}$\n\nHere, $\\mathbf{e}_i$ is the $i$-th basic vector.\n\nThe stopping criterion is usually of the form $||\\nabla f(\\mathbf{x})||_2 \\leq \\nu$.\n\nCoordinate descent optimizes every dimension in turn, for this reason it is sometimes used in minimization problems which enforce sparseness (e.g. LASSO regression).\n\n> *Optimizing one dimension at a time is usually a poor strategy. This is because different dimensions are often related.*\n\n**Assignment 3**\n\n1. Complete the implementation of the coordinate descent method.\n2. Plot the paths for the two toy problems. Use the same stating points as before.\n3. Analyze the convergence."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "\"\"\"\n    coordinate_descent(f, x₀, grad_f; α::Real=0.2, β::Real=0.7,\n        ν::Real=1e-3)\n\nGeneral coordinate descent algorithm.\n\nInputs:\n    - f: function to be minimized\n    - x₀: starting point\n    - grad_f: gradient of the function to be minimized\n    - α: parameter for btls\n    - β: parameter for btls\n    - ν: parameter to determine if the algorithm is converged\n\nOutputs:\n    - xstar: the found minimum\n\"\"\"\nfunction coordinate_descent(f, x₀::Vector, grad_f; α::Real=0.2, β::Real=0.7,\n      ν::Real=1e-7)\n    x = x₀  # initial value\n    Δx = zero(x)\n    nsteps = 0\n    while true\n        Δx .= grad_f(x)  # store gradient here\n        absδ, δ, i =  maximum((abs(δ), δ, i) for (i, δ) in  (enumerate(Δx)))\n        if norm(Δx) < ν\n            break  # converged\n        end\n        Δx .= 0\n        Δx[i] = -δ\n        t = backtracking_line_search(f, x, Δx, grad_f, α=α, β=β)  # BLS for optimal step size\n        x .+= t * Δx  # do a step\n        nsteps += 1\n    end\n    println(\"converged after $nsteps steps\")\n    return x\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "xstarq = coordinate_descent(fquadr, copy(x0q), grad_fquadr)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "xstarnq = coordinate_descent(fnonquadr, copy(x0nq), grad_fnonquadr)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Newton's method\n\n### The Newton step\n\nIn Newton's method the descent direction is chosen as\n\n$$\n\\Delta \\mathbf{x}_\\text{nt} = -(\\nabla^2f(\\mathbf{x}))^{-1} \\nabla f(\\mathbf{x})\\,,\n$$\nwhich is called the *Newton step*.\n\nIf $f$ is convex, then $\\nabla^2f(\\mathbf{x})$ is positive definite and\n$$\n\\nabla f(\\mathbf{x})^\\top \\Delta \\mathbf{\\mathbf{x}}_\\text{nt} \\leq 0\\,,\n$$\nhence the Newton step is a descent direction unless $\\mathbf{x}$ is optimal.\n\nThis Newton step can be motivated in several ways.\n\n**Minimizer of a second order approximation**\n\nThe second order Taylor approximation $\\hat{f}$ of $f$ at $\\mathbf{x}$ is\n\n$$\nf(\\mathbf{x}+\\mathbf{v})\\approx\\hat{f}(\\mathbf{x}+\\mathbf{v}) = f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^\\top \\mathbf{v} + \\frac{1}{2} \\mathbf{v}^\\top \\nabla^2 f(\\mathbf{x}) \\mathbf{v}\\,\n$$\n\nwhich is a convex quadratic function of $\\mathbf{v}$, and is minimized when $\\mathbf{v}=\\Delta \\mathbf{x}_\\text{nt}$.\n\nThis quadratic model will be particularly accurate when $\\mathbf{x}$ is close to $\\mathbf{x}^\\star$.\n\n**Steepest descent direction in Hessian norm**\n\nThe Newton step is the steepest descent step if a quadratic norm using the Hessian is used, i.e.\n$$\n||\\mathbf{u}||_{\\nabla^2f(\\mathbf{x})}=(\\mathbf{u}^\\top\\nabla^2f(\\mathbf{x})\\mathbf{u})^\\frac{1}{2}\\,.\n$$\n\n**Affine invariance of the Newton step**\n\n> *A consistent algorithm should give the same results independent of the units in which quantities are measured.*  ~ Donald Knuth\n\nThe Newton step is independent of linear or affine changes of coordinates. Consider a non-singular $n\\times n$ transformation matrix $T$. If we apply a coordinate transformation $\\mathbf{x}=T\\mathbf{y}$ and define $\\bar{f}(\\mathbf{y}) = f(\\mathbf{x})$, then\n$$\n\\nabla \\bar{f}(\\mathbf{y}) = T^\\top\\nabla f(\\mathbf{x})\\,,\\quad \\nabla^2 \\bar{f}(\\mathbf{y}) = T^\\top\\nabla^2f(\\mathbf{x})T\\,.\n$$\nAs such it follows that\n$$\n\\mathbf{x} + \\Delta \\mathbf{x}_\\text{nt} = T (\\mathbf{y} + \\Delta \\mathbf{y}_\\text{nt})\\,.\n$$\n\n**Questions 2**\n\nDoes scaling and rotation affect the working of gradient descent and coordinate descent?\n\n### Newton decrement\n\nThe Newton decrement is defined as\n$$\n\\lambda(\\mathbf{x})  = (\\nabla f(\\mathbf{x})^\\top\\nabla^2 f(x)^{-1}\\nabla f(\\mathbf{x}))^{1/2}\\,.\n$$\n\nThis can be related to the quantity $f(\\mathbf{x})-\\inf_\\mathbf{y}\\ \\hat{f}(\\mathbf{y})$:\n$$\nf(\\mathbf{x})-\\inf_\\mathbf{y}\\ \\hat{f}(\\mathbf{y}) = f(\\mathbf{x}) - \\hat{f}(\\mathbf{x} +\\Delta \\mathbf{x}_\\text{nt}) = \\frac{1}{2} \\lambda(\\mathbf{x})^2\\,.\n$$\nThus $\\frac{1}{2} \\lambda(\\mathbf{x})^2$ is an estimate of $f(\\mathbf{x}) - p^*$, based on the quadratic approximation of $f$ at $\\mathbf{x}$.\n\n### Pseudocode of Newton's algortihm\n\n>**input** starting point $\\mathbf{x}\\in$ **dom** $f$.\n>\n>**repeat**\n>\n>>    1. Compute the Newton step and decrement $\\Delta \\mathbf{x}_\\text{nt} := -\\nabla^2f(\\mathbf{x})^{-1} \\nabla f(\\mathbf{x})$; $\\lambda^2:=\\nabla f(\\mathbf{x})^\\top\\nabla^2 f(\\mathbf{x})^{-1}\\nabla f(\\mathbf{x})$.\n>>    2. *Stopping criterion* **break** if $\\lambda^2/2 \\leq \\epsilon$.\n>>    3. *Line seach*. Choose a step size $t$ via exact or backtracking line search.\n>>    4. *Update*. $\\mathbf{x}:=\\mathbf{x}+t\\Delta \\mathbf{x}_\\text{nt}$.\n>\n>**output** $\\mathbf{x}$\n\nThe above algorithm is sometimes called the *damped* Newton method, as it uses a variable step size $t$. In practice, using a fixed step also works well. Here, one has to consider the computational cost of using BTLS versus performing a few extra Newton steps to attain the same accuracy.\n\nSee below for the paths of Newton's algorithm on the quadratic and non-quadratic functions. Note that the quadratic problem is solved exactly in one step.\n\n![](Figures/newtons_method.png)\n\nThe following figure shows the convergence of Newton's algorithm on the quadratic and non-quadratic functions. Note that the quadratic problem is solved exactly in one step.\n\n\n### Convergence analysis\n\nIterations in Newton's method fall into two stages:\n\n- *damped Newton phase* $(t < 1)$ until $||\\nabla f(\\mathbf{x})||_2 \\leq \\eta$\n- *pure Newton phase* $(t = 1)$: quadratic convergence\n\nAfter a sufficiently large number of iterations, the number of correct digits doubles at each iteration.\n\n**Assignment 4**\n\n1. Complete the code for Newton's method.\n2. Find the minima of the two toy problems. Use the same starting points as for gradient descent."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "\"\"\"\n    newtons_method(f, x₀, Df, DDf; α::Real=0.2, β::Real=0.7,\n        ϵ::Real=1e-7)\n\nGeneral Newton method.\n\nInputs:\n    - f: function to be minimized\n    - x₀: starting point\n    - Df: gradient of the function to be minimized\n    - DDf: Hessian of the function to be minimized\n    - α: parameter for btls\n    - β: parameter for btls\n    - ϵ: parameter to determine if the algorithm is converged\n\nOutputs:\n    - xstar: the found minimum\n\"\"\"\nfunction newtons_method(f, x₀, Df, DDf; α::Real=0.2, β::Real=0.7,\n      ϵ::Real=1e-5)\n\n    x = x₀  # initial values\n    # preallocation\n    Δx = similar(x)\n    Dfx = similar(x)\n    nsteps = 0\n    while true\n        Dfx .= Df(x) # choose direction\n        Δx .= - DDf(x) \\ Dfx\n        λ² = - Δx' * Dfx  # newton decrement\n        if λ² < ϵ\n            break  # converged\n        end\n        # show to illustrate the change of the regimes\n        @show t = backtracking_line_search(f, x, Δx, Df, α=α, β=β)\n        x .+= t * Δx  # do a step\n        nsteps += 1\n    end\n    println(\"converged after $nsteps steps\")\n    return x\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "xstarq = newtons_method(fquadr, copy(x0q), grad_fquadr, hess_fquadr)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "xstarnq = newtons_method(fnonquadr, copy(x0nq), grad_fnonquadr, hess_fnonquadr)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary Newton's method\n\n- Convergence of Newton's algorithm is rapid and quadratic near $\\mathbf{x}^\\star$.\n- Newton's algorithm is affine invariant, e.g. invariant to choice of coordinates or condition number.\n- Newton's algorithm scales well with problem size. Computationally, computing and storing the Hessian might be prohibitive.\n- The hyperparameters $\\alpha$ and $\\beta$  of BTLS do not influence the performance much.\n\n## Quasi-Newton methods\n\nQuasi-Newton methods try to emulate the success of the Newton method, but without the high computational burden of constructing the Hessian matrix every step. One of the most popular quasi-Newton algorithms is the *Broyden-Fletcher-Goldfarb-Shanno* (BFGS) algorithm. Here, the Hessian is approximated by a symmetric rank-one matrix.\n\n## Illustration on a high-dimensional problem\n\nThe optimization methods on a larger problem:\n\n$$\n\\min_\\mathbf{x} -\\sum_{i=1}^n\\log(1-\\mathbf{x_i}^2) - \\sum_{1=1}^m\\log(b_i - \\mathbf{a_i}^\\top\\mathbf{x})\n$$\n\nwith $n=100$ and $m=1000$."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "n, m = 100, 1000\n\nb = rand(m)\nA = 0.1randn(m, n)\n\nf(x; A=A, b=b) = - sum(log.(max.(1 .- x.*x, 0.0))) - sum(log.(max.(b .- A * x, 0.0)))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the gradient and Hessian using automatic differentiation."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Zygote\n\nDf(x) = f'(x)\nDDf(x) = Zygote.hessian(f, x)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x0 = zeros(n)\n\n@time gradient_descent(f, copy(x0), Df, ν=1e-4)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# takes a really long time...\n@time coordinate_descent(f, copy(x0), Df, ν=1e-3)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@time newtons_method(f, copy(x0), Df, DDf)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise: logistic regression\n\nConsider the following problem: we have a dataset of $n$ instances: $T=\\{(\\mathbf{x}_i, y_i)\\mid i=1\\ldots n\\}$. Here $\\mathbf{x}_i\\in \\mathbb{R}^p$ is a $p$-dimensional feature vector and $y_i\\in\\{0,1\\}$ is a binary label. This is a binary classification problem, we are interested in predicting the label of an instance based on its feature description. The goal of logistic regression is to find a function $f(\\mathbf{x})$ that estimates the conditional probability of $Y$:\n\n$$\n\\mathcal{P}(Y=1 \\mid \\mathbf{X} = \\mathbf{x})\\,.\n$$\n\nWe will assume that this function $f(\\mathbf{x})$ is of the form\n$$\nf(\\mathbf{x}) = \\sigma(\\mathbf{w}^\\top\\mathbf{x})\\,,\n$$\nwith $\\mathbf{w}$ a vector of parameters to be learned and $\\sigma(.)$ the logistic map:\n$$\n\\sigma(t) = \\frac{e^{t}}{1+e^{t}}=\\frac{1}{1+e^{-t}}\\,.\n$$\nIt is easy to see that the logistic mapping will ensure that $f(\\mathbf{x})\\in[0, 1]$, hence $f(\\mathbf{x})$ can be interpreted as a probability.\n\nNote that\n$$\n\\frac{\\text{d}\\sigma(t)}{\\text{d}t} = (1-\\sigma(t))\\sigma(t)\\,.\n$$\n\nTo find the best weights that separate the two classes, we can use the following structured loss function:\n\n$$\n\\mathcal{L}(\\mathbf{w})=-\\sum_{i=1}^n[y_i\\log(\\sigma(\\mathbf{w}^\\top\\mathbf{x}_i))+(1-y_i)\\log(1-\\sigma(\\mathbf{w}^\\top\\mathbf{x}_i))] +\\lambda \\mathbf{w}^\\top\\mathbf{w}\\,.\n$$\n\nHere, the first part is the cross entropy, which penalizes disagreement between the prediction $f(\\mathbf{x}_i)$ and the true label $y_i$, while the second term penalizes complex models in which $\\mathbf{w}$ has a large norm. The trade-off between these two components is controlled by $\\lambda$, a hyperparameter. In the course *Predictive modelling* of Willem Waegeman it is explained that by carefully tuning this parameter one can obtain an improved performance. **In this project we will study the influence $\\lambda$ on the convergence of the optimization algorithms.**\n\n> **Warning**: for this project there is a large risk of numerical problems when computing the loss function. This is because in the cross entropy $0\\log(0)$ should by definition evaluate to its limit value of $0$. Numpy will evaluate this as `nan`. Use the provided function `cross_entropy` which safely computes $-\\sum_{i=1}^n[y_i\\log(\\sigma_i)+(1-y_i)\\log(1-\\sigma_i)]$.\n\n![Toy example in two dimensions illustrating the loss function.](Figures/loss_logistic.png)\n\n**Data overview**\n\nConsider the data set in the file `BreastCancer.csv`. This dataset contains information about 569 female patients diagnosed with breast cancer. For each patient it was recorded wether the tumor was benign (B) or malignant (M), this is the response variable. Each tumor is described by 30 features, which encode some information about the tumor. We want to use logistic regression with regularization to predict wether a tumor is benign or malignant based on these features."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using STMO.CancerData\nfeatures, binary_response = getcancerdata();"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# extract response in binary encoding:\n# 0 : B(enign)\n# 1 : M(alignant)\nbinary_response"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "features"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignments**\n\n1. Derive and implement the loss function for logistic loss, the gradient and the Hessian of this loss function. These functions have as input the parameter vector $\\mathbf{w}$, label vector $\\mathbf{y}$, feature matrix $\\mathbf{X}$ and $\\lambda$. The logistic map and cross-entropy is already provided for you.\n2. Consider $\\lambda=0.1$, find the optimal parameter vector for this data using gradient descent, coordinate descent and Newton's method. Use standardized features. For each algorithm, give the number of steps the algorithm performed and the running time (use the `@elapsed`). Compare the loss for each of parameters obtained by the different algorithms.\n3. How does regularization influence the optimization? Make a separate plot for gradient descent, coordinate descent and Newton's method with the the value of the loss as a function of the iteration of the given algorithm. Make separate the different methods and plot the convergence for $\\lambda = [10^{-3}, 10^{-1}, 1, 10, 100]$. Does increased regularization make the optimization go faster or slower? Why does this make sense?\n\n**Assignment 1**\n\nComplete the functions below."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# COMPLETE THIS FOR QUESTION 1\n\nlogistic_map(x) = 1 / (1.0 + exp(-x));\n# make sure that no nan is returned when p is very small\ncross_entropy(l::Bool, p) = l ? -log(p) : -log(1.0 - p)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "\"\"\"\n    logistic_loss(w, y, X; λ)\n\nImplement the logistic loss\nreturns a scalar\n\"\"\"\nfunction logistic_loss(w, y, X; λ)\n    p = logistic_map.(X * w)\n    return sum(cross_entropy.(y, p)) + λ * w' * w\nend\n\n\"\"\"\n    grad_logistic_loss(w, y, X; λ)\n\nImplement the gradient of the logistic loss\nreturns a column vector\n\"\"\"\nfunction grad_logistic_loss(w, y, X; λ)\n    return X' * (logistic_map.(X * w) .- y) + 2λ * w\nend\n\n\"\"\"\n    hess_logistic_loss(w, y, X; λ)\nImplement the Hessian of the logistic loss\nreturns a matrix\n\"\"\"\nfunction hess_logistic_loss(w, y, X; λ)\n    p = logistic_map.(X * w)\n    return X' * (X .* (p .* (1.0 .- p))) + 2λ * I\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# functions for first question\nl_loss(w) = logistic_loss(w, binary_response, features, λ=0.1);\nl_grad(w) = grad_logistic_loss(w, binary_response, features, λ=0.1);\nl_hess(w) = hess_logistic_loss(w, binary_response, features, λ=0.1);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 2**\n\nUse gradient descent, coordinate descent and Newton's method to find the parameters of the logistic model ($\\lambda=0.1$)."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "w0 = zeros(size(features, 2))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@time wgd = gradient_descent(l_loss, copy(w0), l_grad, ν=1e-5)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@time wgd = coordinate_descent(l_loss, copy(w0), l_grad, ν=1e-5)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@time wnm = newtons_method(l_loss, copy(w0), l_grad, l_hess)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n\n- Boyd, S. and Vandenberghe, L., '*[Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)*'. Cambridge University Press (2004)"
      ],
      "metadata": {}
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.6.1"
    },
    "kernelspec": {
      "name": "julia-1.6",
      "display_name": "Julia 1.6.1",
      "language": "julia"
    }
  },
  "nbformat": 4
}
