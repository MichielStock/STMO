{
  "cells": [
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Plots, LaTeXStrings\nusing STMO"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](Figures/logo.png)\n\n# Motivation\n\nUp to now, we confidently assumed that we would always be able to compute the derivative or gradient of any function. Despite differentiation being a relatively easy operation, it is frequenty not feasible (or desirable) to compute this by hand. *Numerical differentiation* can provide approximations of th derivate or gradient at a particular point. *Automatic differentiation* directly manipulates the computational graph to generate a function that computes the (exact) derivate. Such methods have advanced greatly in the last years and it is no exageration that their easy use in popular software libraries such as TenserFlow and PyTorch are a cornerstone of deep learning and other machine learning and scientific computing fields."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Plots, BenchmarkTools\nusing STMO"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definition of a derivative\n\n$$\n\\frac{\\text{d}f(x)}{\\text{d}x} = f'(x) = \\lim _{h\\to 0}{\\frac {f(x+h)-f(x)}{h}}.\n$$\n\nDerivation is in essence a mechanical process, following the rules below.\n\n![](Figures/derivatives.jpeg)\n\nWhen we work with function of several variables, we use *partial derivatives* (e.g. $\\frac{\\partial f(x, y)}{\\partial x}$), indicating we keep all variables but $x$ fixed.\n\nOur running example:\n\n$$\nf(x) = \\log x + \\frac{\\sin x}{x}\n$$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x) = log(x) + sin(x) / x;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Symbolic differentiation\n\nComputing derivatives, as you have seen in basic calculus courses.\n\nBy hand or automatically:\n- Maple\n- Sympy (python)\n- Mathematica\n- Maxima\n\nDifferentiation is *easy* compared to *integration* or *sampling*.\n\nAdvantages:\n- exact derivatives!\n- gives the formula for different evaluations.\n    - insight in the system\n    - in some cases, closed-form solution extrema by solving $\\frac{\\text{d}f(x)}{\\text{d}x}=0$\n- no hyperparameters or tweaking: just works!\n\nDisadvantages:\n- some software not flexible enough (gradients, arrays, for-loops,...)\n- sometimes explosion of terms: *expression swell*\n- not always numerically optimal!"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using SymEngine"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@vars x  # define variable"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "df = diff(f(x), x)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "df(2.0)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "plot(f, 1, 5, label=\"\\$f(x)\\$\", xlabel=\"\\$x\\$\", lw=2, color=mygreen)\nplot!(df, 1, 5, label=\"\\$f'(x)\\$\", lw=2, color=myorange)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Numerical differentiation\n\nFinite difference approximation of the derivative/gradient based on a number of function evaluations.\n\nOften based on the limit definition of a derivative. Theoretical analysis using Taylor approximation:\n\n$$\nf(x + h) = f(x) + \\frac{h}{1!}f'(x) + \\frac{h^2}{2!}f''(x) + \\frac{h^3}{3!}f^{(3)}(x)+\\ldots\n$$\n\n**Forward difference**\n\n$$\nf'(x)\\approx \\frac{f(x+h) - f(x)}{h}\n$$\n\n**Central difference**\n\n$$\nf'(x)\\approx \\frac{f(x+h) - f(x-h)}{2h}\n$$\n\n**Complex step method**\n\n$$\nf'(x)\\approx \\frac{\\text{Im}(f(x +ih))}{h}\n$$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "diff_fordiff(f, x; h=1e-10) = (f(x + h) - f(x)) / h;\ndiff_centrdiff(f, x; h=1e-10) = (f(x + h) - f(x - h)) / 2h;\ndiff_complstep(f, x; h=1e-10) = imag(f(x + im * h)) / h;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "diff_fordiff(f, 2.0)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "diff_centrdiff(f, 2.0)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "diff_complstep(f, 2.0)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intermezzo: floats\n\nReal numbers are always represented as floating point numbers in a computer.\n\n![Encoding of a real number using a `Float32`.](Figures/floats.png)\n\nBy default, Julia uses double precision floats (`Float64`). For brevity, let us take a look at the bit representation of a float. We use `Float32` for brevity's sake."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "num = Float32(10.789)\nbitstring(num)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first bit encodes the *sign*, here positive."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "sign(num)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next eight bits specify the *exponent*, the magnitude of the number."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "exponent(num)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the final 23 bits specify the *mantissa*, a number between $[1,2]$ representing the precision."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "significand(num)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "These can be used to reconstrunct the number."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "significand(num) * 2^exponent(num)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The *machine precision* of a number can be retained using `eps`. This is the relative error."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "eps(num)  # not very high because it is only Float32"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This means that larger numbers have a larger absolute error compared to small numbers."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "eps(1.2)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "eps(1.2e10)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "eps(1.2e-10)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This brings us with numerical issues we might encounter using numerical differentiation.\n\n**First sin of numerical analysis**:\n\n> *thou shalt not add small numbers to big numbers*\n\n**second sin of numerical analysis**:\n\n> *thou shalt not subtract numbers which are approximately equal*\n\n## Back to numerical differentiation"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "fexamp(x) = 64x*(1-x)*(1-2x)^2*(1-8x+8x^2)^2\ndfexamp = diff(fexamp(x), x)\nerror(diff, h; x=1.0) = max(abs(Float64(dfexamp(x)) - diff(fexamp, x, h=h)), 1e-50);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "stepsizes = map(t->10.0^t, -20:0.1:-1);\nplot(stepsizes, error.(diff_fordiff, stepsizes), label=\"forward difference\",\n    xscale=:log10, yscale=:log10, lw=2, legend=:bottomright, color=myblue)\nplot!(stepsizes, error.(diff_centrdiff, stepsizes), label=\"central difference\", lw=2,\n            color=myred)\nplot!(stepsizes, error.(diff_complstep, stepsizes), label=\"complex step\", lw=2,\n            color=myyellow)\n#xlims!(1e-15, 1e-1)\nxlabel!(\"\\$h\\$\")\nylabel!(\"absolute error\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advantages of numerical differentiation:\n- easy to implement\n- general, no assumptions needed\n\nDisadvantages:\n- not numerically stable (round-off errors)\n- not efficient for gradients ($\\mathcal{O}(n)$ evaluations for $n$-dimensional vectors)\n\n\n## Approximations of multiplications with gradients\n\n**Gradient-vector approximation**\n\n$$\n\\nabla f(\\mathbf{x})^\\intercal \\mathbf{d} \\approx \\frac{f(\\mathbf{x}+h\\cdot\\mathbf{d}) - f(\\mathbf{x}-h\\cdot\\mathbf{d})}{2h}\n$$\n\n**Hessian-vector approximation**\n\n$$\n\\nabla^2 f(\\mathbf{x}) \\mathbf{d} \\approx \\frac{\\nabla f(\\mathbf{x}+h\\cdot\\mathbf{d}) - \\nabla f(\\mathbf{x}-h\\cdot\\mathbf{d})}{2h}\n$$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "grad_vect(f, x, d; h=1e-10) = (f(x + h * d) - f(x - h * d)) / (2h)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "dvect = randn(10) / 10\nxvect = 2rand(10)\n\nA = randn(10, 10)\nA = A * A' / 100\n\n#g(x) = exp(- x' * A * x)  # adjoint does not play with Zygote\ng(x) = exp(- sum(x .* (A * x)))\n\n# correct gradient and Hessian (by hand)\nDg(x) = -2g(x) * A * x\nD²g(x) = -2g(x) * A - 2A * x * Dg(x)'"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "g(xvect)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "Dg(xvect)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "Dg(xvect)' * dvect"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "grad_vect(g, xvect, dvect)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "D²g(xvect) * dvect"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "h = 1e-10\n(Dg(xvect + h * dvect) - Dg(xvect - h * dvect)) / 2h"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forward differentiation\n\nAccumulation of the gradients along the *computational graph*.\n\n<img src=\"Figures/compgraph.png\" alt=\"drawing\" width=\"400\"/>\n\nForward differentiation computes the gradient from the inputs to the outputs.\n\n## Differentiation rules\n\n**Sum rule**:\n\n$$\n\\frac{\\partial (f(x)+g(x))}{\\partial x} =  \\frac{\\partial f(x)}{\\partial x} + \\frac{\\partial f(x)}{\\partial x}\n$$\n\n**Product rule**:\n\n$$\n\\frac{\\partial (f(x)g(x))}{\\partial x} =  f(x)\\frac{\\partial g(x)}{\\partial x} + g(x)\\frac{\\partial f(x)}{\\partial x}\n$$\n\n**Chain rule**:\n\n$$\n\\frac{\\partial (g(f(x))}{\\partial x} =  \\frac{\\partial g(u)}{\\partial u}\\mid_{u=f(x)} \\frac{\\partial f(x)}{\\partial x}\n$$\n\n## Example of the forward differentiation\n\n<img src=\"Figures/forwarddiff.png\" alt=\"drawing\" width=\"600\"/>\n\n## Dual numbers\n\nForward differentiation can be viewed as evaluating function using *dual numbers*, which can be viewed as truncated Taylor series:\n\n$$\nv + \\dot{v}\\epsilon\\,,\n$$\n\nwhere $v,\\dot{v}\\in\\mathbb{R}$ and $\\epsilon$ a nilpotent number, i.e. $\\epsilon^2=0$. For example, we have\n\n$$\n(v + \\dot{v}\\epsilon) + (u + \\dot{u}\\epsilon) = (v+u) + (\\dot{v} +\\dot{u})\\epsilon\n$$\n\n\n$$\n(v + \\dot{v}\\epsilon)(u + \\dot{u}\\epsilon) = (vu) + (v\\dot{u} +\\dot{v}u)\\epsilon\\,.\n$$\n\n\nThese dual numbers can be used as\n\n$$\nf(v+\\dot{v}\\epsilon) = f(v) + f'(v)\\dot{v}\\epsilon\\,.\n$$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "struct Dual{T}\n    v::T\n    vdot::T\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's implement some basic rules showing linearity."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "Base.:+(a::Dual, b::Dual) = Dual(a.v + b.v, a.vdot + b.vdot)\nBase.:*(a::Dual, b::Dual) = Dual(a.v * b.v, a.v * b.vdot + b.v * a.vdot)\nBase.:+(c::Real, b::Dual) = Dual(c + b.v, b.vdot)\nBase.:*(v::Real, b::Dual) = Dual(v, 0.0) * b"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "And some more advanced ones, based on differentiation."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "Base.:sin(a::Dual) = Dual(sin(a.v), cos(a.v) * a.vdot)\nBase.:exp(a::Dual) = Dual(exp(a.v), exp(a.v) * a.vdot)\nBase.:log(a::Dual) = Dual(log(a.v), 1.0 / a.v * a.vdot)\nBase.:/(a::Dual, b::Dual) = Dual(a.v / b.v, (a.vdot * b.v - a.v * b.vdot) / b.v^2)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(Dual(2.0, 1.0))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "myforwarddiff(f, x) = f(Dual(x, 1.0)).vdot\n\nmyforwarddiff(f, 2.0)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This directly works for vectors!"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "q(x) = 10.0 * x[1] * x[2] + x[1] * x[1] + sin(x[1]) / x[2]"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "q([1, 2])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "q(Dual.([1, 2], [1, 0]))  # partial wrt x1"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "q(Dual.([1, 2], [0, 1]))  # partial wrt x2"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practice, we prefer to use a package to do this."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using ForwardDiff"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "ForwardDiff.derivative(f, 2.0)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "ForwardDiff.gradient(g, xvect)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "ForwardDiff.gradient(q, [1, 2])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward differentiation:\n\n- exact gradients!\n- computational complexity scales with **number of inputs**\n- used when you have more outputs than inputs\n\n# Reverse differentiation\n\nCompute the gradient from the output toward the inputs using the chain rule.\n\n<img src=\"Figures/reversediff.png\" alt=\"drawing\" width=\"600\"/>\n\nReverse differentiation:\n\n- also exact!\n- main workhorse for training artificial neural networks.\n- efficient when more inputs than outputs (machine learning: thousands of parameters vs. one loss)"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Zygote"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f'(2.0)  # that's it"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Works as well:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "Zygote.gradient(f, 2.0)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fuctions with more than one variable."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "g'(xvect)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding the Hessian:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "Zygote.hessian(g, xvect)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Artificial neural networks\n\nMulti-layer perceptron.\n\n<img src=\"Figures/ANN_example.png\" alt=\"drawing\" width=\"200\"/>\n\nForward differentiation.\n\n<img src=\"Figures/Forwardprop.png\" alt=\"drawing\" width=\"500\"/>\n\n\nReverse differentation or backpropagation.\n\n<img src=\"Figures/Backprop.png\" alt=\"drawing\" width=\"500\"/>\n\nReturns effect of changing layer output on the loss. Can be related directly to the parameters!\n\n## Exercise: logistic regression\n\nRecall logistic regression on a training set $S=\\{(\\mathbf{x}_i, y_i)\\mid i=1,\\ldots,n\\}$ with $y\\in\\{0,1\\}$.\n\nPrediction:\n\n$$\nf(\\mathbf{x}) = \\sigma(\\mathbf{w}^\\intercal\\mathbf{x})\\,,\n$$\n\nwith $\\sigma(t) = 1 /(1+exp(t))$.\n\nTo find the parameter vector $\\mathbf{w}$, we minimize the cross-entropy:\n\n$$\nL(\\mathbf{w};S)= \\sum_{i=1}^n = - y_i \\log(f(\\mathbf{x})) - (1-y_i)\\log(1-f(\\mathbf{x}))\\,.\n$$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# artificial data\nX = [randn(50, 2); randn(50, 2) .+ [-1.0 2.4]];\ny = [i <= 50 ? 0 : 1 for i in 1:100];\nn = length(y);\n\nscatter(X[:,1], X[:,2], color=y)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "σ(t) = 1.0 / (1.0 + exp(t))\nf(x, w) = σ(sum(x .* w))\nL(w; X=X, y=y) = sum(- y .* log.(σ.(X * w)) - (1.0 .- y) .* log.(1. .- σ.( X * w)))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "w = [0.1, 0.1]\nL(w)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignments**\n\n1. Compute the gradient of $L$ w.r.t. $\\mathbf{w}$ using\n    - numerical method\n    - forward differentiation\n    - backward differentiation\n2. (optional) Implement a simple gradient descent to find  $\\mathbf{w}^\\star$.\n3. Add a bias to the prediction function. Use `Zygote` to compute the gradients w.r.t. both parameters."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Differentiating ODE\n\nAutomatic differentiation can be used beyond machine learning and optimization:\n\n- [physical engines](https://arxiv.org/abs/1611.01652) to learn robot control\n- differentiating [protein](https://github.com/lupoglaz/TorchProteinLibrary) [structures](https://www.cell.com/cell-systems/fulltext/S2405-4712(19)30076-6)\n- Sinkhorn algorithm\n- [dynamic programming](https://arxiv.org/abs/1802.03676)\n- [differential equations](https://julialang.org/blog/2019/01/fluxdiffeq)\n\nEverything is computed by some straightforward and differentiable functions!\n\n# Exercise\n\nConsider the *Wheeler's Ridge* function:\n\n$$\nf(\\mathbf{x}) = -\\exp(-(x_1 x_2 - a)^2 -(x_2 -a)^2)\\,,\n$$\n\nat the point $\\mathbf{x}_0=[1.5, 1.5]^T$. We set $a=1.5$.\n\nImplement this function."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the gradient by hand.\n\nFind the gradient and Hessian at $\\mathbf{x}_0$ by numerical differentiation."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the gradient and Hessian at $\\mathbf{x}_0$ using automatic differentiation."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n\n- Gunes et. al. (2015) *Automatic differentiation in machine learning: a survey*\n- Kochenderfer, M. J. and Wheeler, T., '*Algorithms for Optimization*'. MIT Press (2019)"
      ],
      "metadata": {}
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.6.1"
    },
    "kernelspec": {
      "name": "julia-1.6",
      "display_name": "Julia 1.6.1",
      "language": "julia"
    }
  },
  "nbformat": 4
}
